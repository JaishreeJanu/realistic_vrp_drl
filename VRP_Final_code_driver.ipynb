{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: ENSURE BEFORE RUNNING FINAL EXPERIMENT:\n",
    "\n",
    "- CHECKS: \n",
    "    - Check and update train and test sizes\n",
    "    - Check and update experiment_idx, training params and model params\n",
    "    - Check saving directories are in place and ensure no overwriting of existing files there (due to file_paths where saving\n",
    "    \n",
    "- After experiment run:\n",
    "    - Check all saved results and buffers + model\n",
    "    - Get saved results, observe and document data/plots below\n",
    "    - Generate tours from buffer & results using the function generate_tours\n",
    "    - Document required data points in a final consolidated table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T22:01:46.385853Z",
     "iopub.status.busy": "2023-03-22T22:01:46.384804Z",
     "iopub.status.idle": "2023-03-22T22:02:08.860664Z",
     "shell.execute_reply": "2023-03-22T22:02:08.859611Z",
     "shell.execute_reply.started": "2023-03-22T22:01:46.385818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tianshou\n",
      "  Using cached tianshou-0.5.0-py3-none-any.whl (162 kB)\n",
      "Collecting ray\n",
      "  Using cached ray-2.3.0-cp39-cp39-manylinux2014_x86_64.whl (58.6 MB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.23.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from tianshou) (3.8.0)\n",
      "Requirement already satisfied: tensorboard>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from tianshou) (2.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tianshou) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from tianshou) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tianshou) (23.0)\n",
      "Collecting numba>=0.51.0\n",
      "  Using cached numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "Collecting gymnasium>=0.26.0\n",
      "  Using cached gymnasium-0.27.1-py3-none-any.whl (883 kB)\n",
      "Collecting virtualenv>=20.0.24\n",
      "  Using cached virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from ray) (5.4.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from ray) (4.17.3)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.9/dist-packages (from ray) (1.3.3)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.9/dist-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ray) (2.28.2)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from ray) (18.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from ray) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.9/dist-packages (from ray) (3.19.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from ray) (3.9.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ray) (1.0.4)\n",
      "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from ray) (1.51.1)\n",
      "Collecting jax-jumpy>=0.2.0\n",
      "  Using cached jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium>=0.26.0->tianshou) (4.4.0)\n",
      "Collecting gymnasium-notices>=0.0.1\n",
      "  Using cached gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->tianshou) (66.1.1)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Using cached llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (3.4.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (0.35.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.5.0->tianshou) (1.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->ray) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->ray) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->ray) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->ray) (2.1.1)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Using cached distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.0.24->ray) (2.6.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->ray) (0.19.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou) (1.14.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.5.0->tianshou) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.5.0->tianshou) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.5.0->tianshou) (3.2.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827631 sha256=672de69c4d8696241fd1d81b1469d4128819dcd58c6ac84d454ab5c855d52992\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/bf/16/63ad354aa94e522b5fddf1729d8dfc47ad5980079054d4f714\n",
      "Successfully built gym\n",
      "Installing collected packages: gymnasium-notices, gym-notices, distlib, virtualenv, llvmlite, jax-jumpy, ray, numba, gymnasium, gym, tianshou\n",
      "Successfully installed distlib-0.3.6 gym-0.26.2 gym-notices-0.0.8 gymnasium-0.27.1 gymnasium-notices-0.0.1 jax-jumpy-1.0.0 llvmlite-0.39.1 numba-0.56.4 ray-2.3.0 tianshou-0.5.0 virtualenv-20.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from functools import partial\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from ray import tune\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T22:02:28.686902Z",
     "iopub.status.busy": "2023-03-22T22:02:28.686343Z",
     "iopub.status.idle": "2023-03-22T22:02:30.664790Z",
     "shell.execute_reply": "2023-03-22T22:02:30.663568Z",
     "shell.execute_reply.started": "2023-03-22T22:02:28.686868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modules from tianshou framework\n",
    "\n",
    "import tianshou\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union, Dict\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.data import Batch, to_torch, to_torch_as\n",
    "from tianshou.policy import BasePolicy\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "\n",
    "from tianshou.env.worker import (\n",
    "    DummyEnvWorker,\n",
    "    EnvWorker,\n",
    "    RayEnvWorker,\n",
    "    SubprocEnvWorker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T22:02:33.910711Z",
     "iopub.status.busy": "2023-03-22T22:02:33.910300Z",
     "iopub.status.idle": "2023-03-22T22:02:34.159428Z",
     "shell.execute_reply": "2023-03-22T22:02:34.158519Z",
     "shell.execute_reply.started": "2023-03-22T22:02:33.910673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Derived modules and custom defined classes\n",
    "\n",
    "from env.VRPEnv import VRPEnv\n",
    "from policy.VRPPolicy import REINFORCEPolicy\n",
    "from nets.attention_model_D2 import AttentionModel\n",
    "\n",
    "from data.VRPCollector import Collector\n",
    "from data.BufferManager import ReplayBuffer, VectorReplayBuffer\n",
    "from policy.VRPtrainer import OnpolicyTrainer, onpolicy_trainer\n",
    "from data.Graph_Viz import decode_buffer, plot_vehicle_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T22:02:37.578824Z",
     "iopub.status.busy": "2023-03-22T22:02:37.578502Z",
     "iopub.status.idle": "2023-03-22T22:02:37.583792Z",
     "shell.execute_reply": "2023-03-22T22:02:37.583002Z",
     "shell.execute_reply.started": "2023-03-22T22:02:37.578794Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.get_device_properties(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T22:03:36.269033Z",
     "iopub.status.busy": "2023-03-22T22:03:36.268696Z",
     "iopub.status.idle": "2023-03-22T22:03:36.275050Z",
     "shell.execute_reply": "2023-03-22T22:03:36.274075Z",
     "shell.execute_reply.started": "2023-03-22T22:03:36.269006Z"
    }
   },
   "outputs": [],
   "source": [
    "## Parameters of dataset\n",
    "def load_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load the saved dataset/graphs\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_path = data_dir + \"/train/train_graphs_50000x20.pickle\"\n",
    "    test_data_path = data_dir + \"/test/test_graphs_10000x20.pickle\"\n",
    "\n",
    "    with open(train_data_path, 'rb') as train_handle:\n",
    "        load_train_graphs = pickle.load(train_handle)\n",
    "\n",
    "    with open(test_data_path, 'rb') as test_handle:\n",
    "        load_test_graphs = pickle.load(test_handle) \n",
    "        \n",
    "    return load_train_graphs, load_test_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T22:03:38.574229Z",
     "iopub.status.busy": "2023-03-22T22:03:38.573357Z",
     "iopub.status.idle": "2023-03-22T22:03:58.162455Z",
     "shell.execute_reply": "2023-03-22T22:03:58.161137Z",
     "shell.execute_reply.started": "2023-03-22T22:03:38.574199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_idx': 'A10x10', 'learning_rate': 0.0001, 'betas': (0.9, 0.99), 'weight_decay': 0.01, 'n_epochs': 30, 'batch_size': 64, 'embedding_dim': 64, 'hidden_dim': 16, 'n_encode_layers': 2, 'graph_size': 20, 'train_graphs': 50000, 'test_graphs': 10000, 'train_buffer_size': 100000, 'test_buffer_size': 100000, 'repeat_per_collect': 1, 'test_in_train': True, 'episode_per_collect': 50000, 'episode_per_test': 50000, 'step_per_epoch': 1000000}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type or tuple of types",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m VRPpolicy \u001b[38;5;241m=\u001b[39m REINFORCEPolicy(model, optim)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Setting up Vectorized environments for train and test datasets\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m train_envs \u001b[38;5;241m=\u001b[39m \u001b[43mDummyVectorEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mVRPEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mload_train_graphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m test_envs \u001b[38;5;241m=\u001b[39m DummyVectorEnv([\u001b[38;5;28;01mlambda\u001b[39;00m instance\u001b[38;5;241m=\u001b[39mgraph, idx\u001b[38;5;241m=\u001b[39mi: VRPEnv(instance, idx) \u001b[38;5;28;01mfor\u001b[39;00m i,graph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(load_test_graphs)])\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Setting up Replay Buffers and Collectors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tianshou/env/venvs.py:443\u001b[0m, in \u001b[0;36mDummyVectorEnv.__init__\u001b[0;34m(self, env_fns, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], ENV_TYPE]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDummyEnvWorker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tianshou/env/venvs.py:153\u001b[0m, in \u001b[0;36mBaseVectorEnv.__init__\u001b[0;34m(self, env_fns, worker_fn, wait_num, timeout)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# A VectorEnv contains a pool of EnvWorkers, which corresponds to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# interact with the given envs (one worker <-> one env).\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m [worker_fn(_patch_env_generator(fn)) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class, EnvWorker)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tianshou/env/venvs.py:153\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# A VectorEnv contains a pool of EnvWorkers, which corresponds to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# interact with the given envs (one worker <-> one env).\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m [\u001b[43mworker_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_patch_env_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class, EnvWorker)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tianshou/env/worker/dummy.py:13\u001b[0m, in \u001b[0;36mDummyEnvWorker.__init__\u001b[0;34m(self, env_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fn: Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43menv_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env_fn)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tianshou/env/venvs.py:49\u001b[0m, in \u001b[0;36m_patch_env_generator.<locals>.patched\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m callable(\n\u001b[1;32m     45\u001b[0m     fn\n\u001b[1;32m     46\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv generators that are provided to vector environemnts must be callable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m env \u001b[38;5;241m=\u001b[39m fn()\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPettingZooEnv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_old_gym \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, old_gym\u001b[38;5;241m.\u001b[39mEnv):\n",
      "\u001b[0;31mTypeError\u001b[0m: isinstance() arg 2 must be a type or tuple of types"
     ]
    }
   ],
   "source": [
    "# Training process setup\n",
    "data_dir = \"./data\"\n",
    "load_train_graphs, load_test_graphs = load_data(data_dir)\n",
    "\n",
    "# Training parameters\n",
    "training_params = {\n",
    "\"experiment_idx\" : \"A10x10\",\n",
    "\n",
    "# Optimization\n",
    "\"learning_rate\" : 0.0001,\n",
    "\"betas\" : (0.9, 0.99), # coefficients used for computing running averages of gradient and its square\n",
    "\"weight_decay\" : 0.01,  # weight decay coefficient for regularization\n",
    "\"n_epochs\" : 30,\n",
    "\"batch_size\" : 64,\n",
    "\n",
    "#Model configuration\n",
    "\"embedding_dim\" : 64,\n",
    "\"hidden_dim\" : 16,\n",
    "\"n_encode_layers\" : 2,\n",
    "\n",
    "# Trainer and Collector setup (will remain almost same, increase buffer sizes for larger datasets)\n",
    "\"graph_size\" : load_train_graphs[0][\"node_features\"].shape[0] - 1,\n",
    "\"train_graphs\" : len(load_train_graphs),\n",
    "\"test_graphs\" : len(load_test_graphs),\n",
    "\"train_buffer_size\" : 100000,\n",
    "\"test_buffer_size\" : 100000,\n",
    "\"repeat_per_collect\" : 1,\n",
    "\"test_in_train\" : True}\n",
    "\n",
    "training_params[\"episode_per_collect\"] = training_params[\"episode_per_test\"] = training_params[\"train_graphs\"]\n",
    "training_params[\"step_per_epoch\"] = training_params[\"graph_size\"] * training_params[\"train_graphs\"]\n",
    "\n",
    "print(training_params)\n",
    "\n",
    "\n",
    "model = AttentionModel(\n",
    "    embedding_dim = training_params[\"embedding_dim\"],\n",
    "    hidden_dim = training_params[\"hidden_dim\"],\n",
    "    n_encode_layers = training_params[\"n_encode_layers\"],\n",
    "    graph_size = training_params[\"graph_size\"],\n",
    "    tanh_clipping = 10,\n",
    "    mask_inner = True, \n",
    "    mask_logits = True,\n",
    "    normalization = 'batch',\n",
    "    n_heads = 8,\n",
    "    checkpoint_encoder = False,\n",
    "    shrink_size = None)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr = training_params[\"learning_rate\"],\n",
    "    betas = training_params[\"betas\"], \n",
    "    weight_decay = training_params[\"weight_decay\"],\n",
    "    eps = 1e-08)\n",
    "\n",
    "VRPpolicy = REINFORCEPolicy(model, optim)\n",
    "\n",
    "\n",
    "# Setting up Vectorized environments for train and test datasets\n",
    "train_envs = DummyVectorEnv([lambda instance=graph, idx=i: VRPEnv(instance, idx) for i,graph in enumerate(load_train_graphs)])\n",
    "test_envs = DummyVectorEnv([lambda instance=graph, idx=i: VRPEnv(instance, idx) for i,graph in enumerate(load_test_graphs)])\n",
    "\n",
    "# Setting up Replay Buffers and Collectors\n",
    "test_replaybuffer = VectorReplayBuffer(training_params[\"test_buffer_size\"], buffer_num=training_params[\"test_graphs\"])\n",
    "train_replaybuffer = VectorReplayBuffer(training_params[\"train_buffer_size\"], buffer_num=training_params[\"train_graphs\"])\n",
    "test_collector = Collector(VRPpolicy, test_envs, test_replaybuffer)\n",
    "train_collector = Collector(VRPpolicy, train_envs, train_replaybuffer)\n",
    "\n",
    "# Setting up trainer \n",
    "logdir = \"./logs/\"\n",
    "exp_num = training_params[\"experiment_idx\"]\n",
    "# Setup Tensorboard logger\n",
    "log_path = os.path.join(logdir, f\"VRPtraining_exp{exp_num}\")\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)\n",
    "train_collector.reset()\n",
    "test_collector.reset()\n",
    "train_replaybuffer.reset()\n",
    "test_replaybuffer.reset()\n",
    "trainer = OnpolicyTrainer(\n",
    "    VRPpolicy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch = training_params[\"n_epochs\"],\n",
    "    step_per_epoch = training_params[\"step_per_epoch\"],\n",
    "    repeat_per_collect = training_params[\"repeat_per_collect\"],\n",
    "    episode_per_test = training_params[\"episode_per_test\"],\n",
    "    episode_per_collect = training_params[\"episode_per_collect\"],\n",
    "    batch_size = training_params[\"batch_size\"],\n",
    "    logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-22T22:03:58.163290Z",
     "iopub.status.idle": "2023-03-22T22:03:58.163643Z",
     "shell.execute_reply": "2023-03-22T22:03:58.163490Z",
     "shell.execute_reply.started": "2023-03-22T22:03:58.163472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model and store epoch stats in a dataframe\n",
    "losses = []\n",
    "train_stat = []\n",
    "for epoch, epoch_stat, info in trainer:\n",
    "    losses.append(-epoch_stat[\"loss\"])\n",
    "    epoch_stat[\"epoch\"] = epoch\n",
    "    train_stat.append(epoch_stat)\n",
    "    print(\"\\n\", epoch_stat)\n",
    "    \n",
    "    #with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "    #    path = os.path.join(checkpoint_dir, \"checkpoints\")\n",
    "    #    torch.save((model.state_dict(), optim.state_dict()), path)\n",
    "        \n",
    "#train_df_cols = epoch_stat.keys()\n",
    "#train_df = pd.DataFrame(train_stat, columns = train_df_cols)\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model and results\n",
    "\n",
    "exp_idx = f\"model_exp{exp_num}_g{training_params['graph_size']}_train{len(load_train_graphs)}_test{len(load_test_graphs)}\"\n",
    "file_path = \"./trained_models/models/\" + f\"{exp_idx}.pth\"\n",
    "torch.save(model.state_dict(), file_path)\n",
    "\n",
    "\n",
    "# Collecting test and train results and buffers data\n",
    "train_result = train_collector.collect(n_episode=len(load_train_graphs))\n",
    "test_result = test_collector.collect(n_episode=len(load_test_graphs))\n",
    "train_buffer_df = decode_buffer(train_replaybuffer)\n",
    "test_buffer_df = decode_buffer(test_replaybuffer)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(f\"Train Results: {test_result}\")\n",
    "print(f\"Test Results: {test_result}\")\n",
    "\n",
    "res_path = \"./trained_models/results/\"\n",
    "#train_df.to_csv(res_path + f\"train_df_{exp_idx}\", index=False)\n",
    "train_buffer_df.to_csv(res_path + f\"train_bufferdf_{exp_idx}\", index=False)\n",
    "test_buffer_df.to_csv(res_path + f\"test_bufferdf_{exp_idx}\", index=False)\n",
    "\n",
    "\n",
    "with open(res_path + f\"test_result_{exp_idx}.pickle\", 'wb') as handle: \n",
    "        pickle.dump(test_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "with open(res_path + f\"train_result_{exp_idx}.pickle\", 'wb') as handle: \n",
    "        pickle.dump(train_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "with open(res_path + f\"params_{exp_idx}.pickle\", 'wb') as handle: \n",
    "        pickle.dump(training_params, handle, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### EXECUTE TRAINING SCRIPT #################################################\n",
    "#! python \"final_model_training_A1.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING SAVED OBJECTS -- USE IF AND WHEN REQUIRED\n",
    "\n",
    "# Loading the results for model you wanna explore results\n",
    "#graph_size = 10\n",
    "#train_size = 100000\n",
    "#test_size = 100000\n",
    "#\n",
    "## Save directories\n",
    "#exp_idx = f\"model_expA1_g{graph_size}_train{train_size}_test{test_size}\"\n",
    "#m_dir = \"./trained_models/models/\"\n",
    "#res_dir = \"./trained_models/results/\"\n",
    "#\n",
    "## Loading training params\n",
    "#with open(res_dir + \"params_\" + exp_idx, 'rb') as handle:\n",
    "#    training_params = pickle.load(handle)\n",
    "#\n",
    "#    \n",
    "## Loading saved model\n",
    "#model = AttentionModel(\n",
    "#        embedding_dim=training_params[\"embedding_dim\"],\n",
    "#        hidden_dim=training_params[\"hidden_dim\"],\n",
    "#        graph_size = training_params[\"graph_size\"],\n",
    "#        n_encode_layers=training_params[\"n_encode_layers\"],\n",
    "#        tanh_clipping=10.,\n",
    "#        mask_inner=True, \n",
    "#        mask_logits=True,\n",
    "#        normalization='batch',\n",
    "#        n_heads=8,\n",
    "#        checkpoint_encoder=False,\n",
    "#        shrink_size=None)\n",
    "#\n",
    "#model.load_state_dict(torch.load(m_dir + f\"{exp_idx}.pth\"))\n",
    "#\n",
    "#\n",
    "## Loading saved results\n",
    "#with open(res_dir + \"test_result_\" + exp_idx, 'rb') as handle:\n",
    "#    train_result = pickle.load(handle)\n",
    "#    \n",
    "#with open(res_dir + \"test_result_\" + exp_idx, 'rb') as handle:\n",
    "#    train_result = pickle.load(handle)\n",
    "#    \n",
    "#train_df = pd.read_csv(res_dir + f\"train_df_{exp_idx}\")\n",
    "#train_buffer_df = pd.read_csv(res_dir + f\"train_bufferdf_{exp_idx}\")\n",
    "#test_buffer_df = pd.read_csv(res_dir + f\"test_bufferdf_{exp_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test result\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_buffer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss and avg rewards over iterations\n",
    "\n",
    "n_epochs = training_params[\"n_epochs\"]\n",
    "train_df[\"loss\"] = losses\n",
    "x = [e for e in range (n_epochs)]\n",
    "default_x_ticks = range(len(x))\n",
    "fig = plt.figure(figsize=(20 ,5))\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "train_df[\"loss\"].plot(style='o--', label=\"train loss\")\n",
    "train_df['loss'].expanding().mean().plot(style='k-', label=\"cumm_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.xticks(default_x_ticks, x, rotation=20)\n",
    "plt.ylabel(\"training_loss\")\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "train_df[\"rew\"].plot(style='bo--', label=\"avg train reward\")\n",
    "train_df['rew'].expanding().mean().plot(style='k-', label=\"cumm_rew\")\n",
    "train_df[\"bl_rew\"].plot(style='r--', label=\"baseline reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.xticks(default_x_ticks, x, rotation=20)\n",
    "plt.ylabel(\"training vs baseline average rewards\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch-wise training stats\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studying and Plotting collected train and test solutions (distance values are abs(rewards))\n",
    "\n",
    "def get_distances(rewards):\n",
    "    distances = np.array([round(abs(rew),3) for rew in rewards])\n",
    "    return distances\n",
    "\n",
    "\n",
    "# Get computed distance values from the reward list\n",
    "test_distances = get_distances(test_result[\"rews\"])\n",
    "test_bl_distances = get_distances(test_result[\"bl_rews\"])\n",
    "\n",
    "train_distances = get_distances(train_result[\"rews\"])\n",
    "train_bl_distances = get_distances(train_result[\"bl_rews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_rew, best_test_rew  = round(np.mean(test_distances, axis=0),3), np.min(test_distances, axis=0)\n",
    "avg_test_rew_bl, best_test_rew_bl = round(np.mean(test_bl_distances, axis=0),3), np.min(test_bl_distances, axis=0)\n",
    "\n",
    "\n",
    "avg_train_rew, best_train_rew  = round(np.mean(train_distances, axis=0),3), np.min(train_distances, axis=0)\n",
    "avg_train_rew_bl, best_train_rew_bl = round(np.mean(train_bl_distances, axis=0),3), np.min(train_bl_distances, axis=0)\n",
    "\n",
    "\n",
    "print(\"TEST RESULTS\")\n",
    "print(f\"Mean test reward: {avg_test_rew}\")\n",
    "print(f\"Best test reward: {best_test_rew}\")\n",
    "print(f\"\\nMean baseline reward: {avg_test_rew_bl}\")\n",
    "print(f\"Best baseline reward: {best_test_rew_bl}\")\n",
    "\n",
    "print(\"\\n----------------\\n\")\n",
    "print(\"TRAIN RESULTS\")\n",
    "print(f\"Mean train reward: {avg_train_rew}\")\n",
    "print(f\"Best train reward: {best_train_rew}\")\n",
    "print(f\"\\nMean baseline reward: {avg_train_rew_bl}\")\n",
    "print(f\"Best baseline reward: {best_train_rew_bl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test and train distance values\n",
    "\n",
    "fig = plt.figure(figsize=(15 ,8))\n",
    "\n",
    "pd.Series(train_distances).plot.kde(style='b-', label=\"train rewards\")\n",
    "pd.Series(test_distances).plot.kde(style='r-', label=\"test rewards\")\n",
    "\n",
    "pd.Series(train_bl_distances).plot.kde(style='b--', label=\"train baseline rewards\")\n",
    "pd.Series(test_bl_distances).plot.kde(style='r--', label=\"test baseline rewards\")\n",
    "\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(\"Model vs Baseline Distances (Test/Train)\", size=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap between test distances and corresponding baseline solutions\n",
    "fig = plt.figure(figsize=(8 ,5)) \n",
    "\n",
    "gaps = []\n",
    "for i, dist in enumerate(test_distances):\n",
    "    gap = (test_distances[i] - test_bl_distances[i]) / test_bl_distances[i]\n",
    "    gaps.append(gap)\n",
    "    \n",
    "    \n",
    "gaps = np.sort(np.array(gaps))\n",
    "plt.hist(gaps, bins=10)\n",
    "#plt.plot(base)\n",
    "\n",
    "plt.xlabel(\"Sorting index\")\n",
    "plt.ylabel(\"Gap % vs Greedy Baseline\")\n",
    "plt.title(\"Test Distances Gap\", size=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gaps -- greedy vs experiment\n",
    "\n",
    "test_rewards_df = test_buffer_df.groupby(['env_id']).agg(action_count=('action', 'count'), \n",
    "                                        reward=('reward', 'sum'), \n",
    "                                       bl_reward=('bl_reward', 'sum'))\n",
    "\n",
    "\n",
    "test_rewards_df[\"avg_reward\"] = test_rewards_df[\"reward\"] * training_params[\"graph_size\"] / test_rewards_df[\"action_count\"]\n",
    "test_rewards_df[\"avg_bl_reward\"] = test_rewards_df[\"bl_reward\"] * training_params[\"graph_size\"] / test_rewards_df[\"action_count\"]\n",
    "test_rewards_df[\"reward_gap\"] = test_rewards_df[\"avg_reward\"] - test_rewards_df[\"avg_bl_reward\"]\n",
    "test_rewards_df[\"gap\"] = test_rewards_df[\"reward_gap\"]*100 / test_rewards_df[\"avg_bl_reward\"]\n",
    "\n",
    "test_rewards_df.sort_values(\"gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tours, bl_tours = generate_tours(test_buffer_df, test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "\n",
    "graph_data = load_test_graphs[idx]\n",
    "graph_route = tours[idx]\n",
    "graph_route_bl = bl_tours[idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_vehicle_routes(graph_data, graph_route, ax, visualize_demands=False, demand_scale=50, round_demand=True)\n",
    "\n",
    "#plt.subplot(122)\n",
    "#plot_vehicle_routes(graph_data, graph_route_bl, ax, visualize_demands=False, demand_scale=50, round_demand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_vehicle_routes(graph_data, graph_route_bl, ax, visualize_demands=False, demand_scale=50, round_demand=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
