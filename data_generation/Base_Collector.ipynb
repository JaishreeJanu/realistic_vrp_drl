{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tianshou.data import (\n",
    "    Batch,\n",
    "    CachedReplayBuffer,\n",
    "    ReplayBuffer,\n",
    "    ReplayBufferManager,\n",
    "    VectorReplayBuffer,\n",
    "    to_numpy,\n",
    ")\n",
    "from tianshou.data.batch import _alloc_by_keys_diff\n",
    "from tianshou.env import BaseVectorEnv, DummyVectorEnv\n",
    "from tianshou.policy import BasePolicy\n",
    "\n",
    "\n",
    "class Collector(object):\n",
    "    \"\"\"Collector enables the policy to interact with different types of envs with \\\n",
    "    exact number of steps or episodes.\n",
    "\n",
    "    :param policy: an instance of the :class:`~tianshou.policy.BasePolicy` class.\n",
    "    :param env: a ``gym.Env`` environment or an instance of the\n",
    "        :class:`~tianshou.env.BaseVectorEnv` class.\n",
    "    :param buffer: an instance of the :class:`~tianshou.data.ReplayBuffer` class.\n",
    "        If set to None, it will not store the data. Default to None.\n",
    "    :param function preprocess_fn: a function called before the data has been added to\n",
    "        the buffer, see issue #42 and :ref:`preprocess_fn`. Default to None.\n",
    "    :param bool exploration_noise: determine whether the action needs to be modified\n",
    "        with corresponding policy's exploration noise. If so, \"policy.\n",
    "        exploration_noise(act, batch)\" will be called automatically to add the\n",
    "        exploration noise into action. Default to False.\n",
    "\n",
    "    The \"preprocess_fn\" is a function called before the data has been added to the\n",
    "    buffer with batch format. It will receive only \"obs\" and \"env_id\" when the\n",
    "    collector resets the environment, and will receive the keys \"obs_next\", \"rew\",\n",
    "    \"terminated\", \"truncated, \"info\", \"policy\" and \"env_id\" in a normal env step.\n",
    "    Alternatively, it may also accept the keys \"obs_next\", \"rew\", \"done\", \"info\",\n",
    "    \"policy\" and \"env_id\".\n",
    "    It returns either a dict or a :class:`~tianshou.data.Batch` with the modified\n",
    "    keys and values. Examples are in \"test/base/test_collector.py\".\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        Please make sure the given environment has a time limitation if using n_episode\n",
    "        collect option.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        In past versions of Tianshou, the replay buffer that was passed to `__init__`\n",
    "        was automatically reset. This is not done in the current implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: BasePolicy,\n",
    "        env: Union[gym.Env, BaseVectorEnv],\n",
    "        buffer: Optional[ReplayBuffer] = None,\n",
    "        preprocess_fn: Optional[Callable[..., Batch]] = None,\n",
    "        exploration_noise: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(env, gym.Env) and not hasattr(env, \"__len__\"):\n",
    "            warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n",
    "            self.env = DummyVectorEnv([lambda: env])  # type: ignore\n",
    "        else:\n",
    "            self.env = env  # type: ignore\n",
    "        self.env_num = len(self.env)\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self._assign_buffer(buffer)\n",
    "        self.policy = policy\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self._action_space = self.env.action_space\n",
    "        # avoid creating attribute outside __init__\n",
    "        self.reset(False)\n",
    "\n",
    "    def _assign_buffer(self, buffer: Optional[ReplayBuffer]) -> None:\n",
    "        \"\"\"Check if the buffer matches the constraint.\"\"\"\n",
    "        if buffer is None:\n",
    "            buffer = VectorReplayBuffer(self.env_num, self.env_num)\n",
    "        elif isinstance(buffer, ReplayBufferManager):\n",
    "            assert buffer.buffer_num >= self.env_num\n",
    "            if isinstance(buffer, CachedReplayBuffer):\n",
    "                assert buffer.cached_buffer_num >= self.env_num\n",
    "        else:  # ReplayBuffer or PrioritizedReplayBuffer\n",
    "            assert buffer.maxsize > 0\n",
    "            if self.env_num > 1:\n",
    "                if type(buffer) == ReplayBuffer:\n",
    "                    buffer_type = \"ReplayBuffer\"\n",
    "                    vector_type = \"VectorReplayBuffer\"\n",
    "                else:\n",
    "                    buffer_type = \"PrioritizedReplayBuffer\"\n",
    "                    vector_type = \"PrioritizedVectorReplayBuffer\"\n",
    "                raise TypeError(\n",
    "                    f\"Cannot use {buffer_type}(size={buffer.maxsize}, ...) to collect \"\n",
    "                    f\"{self.env_num} envs,\\n\\tplease use {vector_type}(total_size=\"\n",
    "                    f\"{buffer.maxsize}, buffer_num={self.env_num}, ...) instead.\"\n",
    "                )\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        reset_buffer: bool = True,\n",
    "        gym_reset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Reset the environment, statistics, current data and possibly replay memory.\n",
    "\n",
    "        :param bool reset_buffer: if true, reset the replay buffer that is attached\n",
    "            to the collector.\n",
    "        :param gym_reset_kwargs: extra keyword arguments to pass into the environment's\n",
    "            reset function. Defaults to None (extra keyword arguments)\n",
    "        \"\"\"\n",
    "        # use empty Batch for \"state\" so that self.data supports slicing\n",
    "        # convert empty Batch to None when passing data to policy\n",
    "        self.data = Batch(\n",
    "            obs={},\n",
    "            act={},\n",
    "            rew={},\n",
    "            terminated={},\n",
    "            truncated={},\n",
    "            done={},\n",
    "            obs_next={},\n",
    "            info={},\n",
    "            policy={}\n",
    "        )\n",
    "        self.reset_env(gym_reset_kwargs)\n",
    "        if reset_buffer:\n",
    "            self.reset_buffer()\n",
    "        self.reset_stat()\n",
    "\n",
    "    def reset_stat(self) -> None:\n",
    "        \"\"\"Reset the statistic variables.\"\"\"\n",
    "        self.collect_step, self.collect_episode, self.collect_time = 0, 0, 0.0\n",
    "\n",
    "    def reset_buffer(self, keep_statistics: bool = False) -> None:\n",
    "        \"\"\"Reset the data buffer.\"\"\"\n",
    "        self.buffer.reset(keep_statistics=keep_statistics)\n",
    "\n",
    "    def reset_env(self, gym_reset_kwargs: Optional[Dict[str, Any]] = None) -> None:\n",
    "        \"\"\"Reset all of the environments.\"\"\"\n",
    "        gym_reset_kwargs = gym_reset_kwargs if gym_reset_kwargs else {}\n",
    "        rval = self.env.reset(**gym_reset_kwargs)\n",
    "        returns_info = isinstance(rval, (tuple, list)) and len(rval) == 2 and (\n",
    "            isinstance(rval[1], dict) or isinstance(rval[1][0], dict)\n",
    "        )\n",
    "        if returns_info:\n",
    "            obs, info = rval\n",
    "            if self.preprocess_fn:\n",
    "                processed_data = self.preprocess_fn(\n",
    "                    obs=obs, info=info, env_id=np.arange(self.env_num)\n",
    "                )\n",
    "                obs = processed_data.get(\"obs\", obs)\n",
    "                info = processed_data.get(\"info\", info)\n",
    "            self.data.info = info\n",
    "        else:\n",
    "            obs = rval\n",
    "            if self.preprocess_fn:\n",
    "                obs = self.preprocess_fn(obs=obs, env_id=np.arange(self.env_num\n",
    "                                                                   )).get(\"obs\", obs)\n",
    "        self.data.obs = obs\n",
    "\n",
    "    def _reset_state(self, id: Union[int, List[int]]) -> None:\n",
    "        \"\"\"Reset the hidden state: self.data.state[id].\"\"\"\n",
    "        if hasattr(self.data.policy, \"hidden_state\"):\n",
    "            state = self.data.policy.hidden_state  # it is a reference\n",
    "            if isinstance(state, torch.Tensor):\n",
    "                state[id].zero_()\n",
    "            elif isinstance(state, np.ndarray):\n",
    "                state[id] = None if state.dtype == object else 0\n",
    "            elif isinstance(state, Batch):\n",
    "                state.empty_(id)\n",
    "\n",
    "    def _reset_env_with_ids(\n",
    "        self,\n",
    "        local_ids: Union[List[int], np.ndarray],\n",
    "        global_ids: Union[List[int], np.ndarray],\n",
    "        gym_reset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        gym_reset_kwargs = gym_reset_kwargs if gym_reset_kwargs else {}\n",
    "        rval = self.env.reset(global_ids, **gym_reset_kwargs)\n",
    "        returns_info = isinstance(rval, (tuple, list)) and len(rval) == 2 and (\n",
    "            isinstance(rval[1], dict) or isinstance(rval[1][0], dict)\n",
    "        )\n",
    "        if returns_info:\n",
    "            obs_reset, info = rval\n",
    "            if self.preprocess_fn:\n",
    "                processed_data = self.preprocess_fn(\n",
    "                    obs=obs_reset, info=info, env_id=global_ids\n",
    "                )\n",
    "                obs_reset = processed_data.get(\"obs\", obs_reset)\n",
    "                info = processed_data.get(\"info\", info)\n",
    "            self.data.info[local_ids] = info\n",
    "        else:\n",
    "            obs_reset = rval\n",
    "            if self.preprocess_fn:\n",
    "                obs_reset = self.preprocess_fn(obs=obs_reset, env_id=global_ids\n",
    "                                               ).get(\"obs\", obs_reset)\n",
    "        self.data.obs_next[local_ids] = obs_reset\n",
    "\n",
    "    def collect(\n",
    "        self,\n",
    "        n_step: Optional[int] = None,\n",
    "        n_episode: Optional[int] = None,\n",
    "        random: bool = False,\n",
    "        render: Optional[float] = None,\n",
    "        no_grad: bool = True,\n",
    "        gym_reset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Collect a specified number of step or episode.\n",
    "\n",
    "        To ensure unbiased sampling result with n_episode option, this function will\n",
    "        first collect ``n_episode - env_num`` episodes, then for the last ``env_num``\n",
    "        episodes, they will be collected evenly from each env.\n",
    "\n",
    "        :param int n_step: how many steps you want to collect.\n",
    "        :param int n_episode: how many episodes you want to collect.\n",
    "        :param bool random: whether to use random policy for collecting data. Default\n",
    "            to False.\n",
    "        :param float render: the sleep time between rendering consecutive frames.\n",
    "            Default to None (no rendering).\n",
    "        :param bool no_grad: whether to retain gradient in policy.forward(). Default to\n",
    "            True (no gradient retaining).\n",
    "        :param gym_reset_kwargs: extra keyword arguments to pass into the environment's\n",
    "            reset function. Defaults to None (extra keyword arguments)\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            One and only one collection number specification is permitted, either\n",
    "            ``n_step`` or ``n_episode``.\n",
    "\n",
    "        :return: A dict including the following keys\n",
    "\n",
    "            * ``n/ep`` collected number of episodes.\n",
    "            * ``n/st`` collected number of steps.\n",
    "            * ``rews`` array of episode reward over collected episodes.\n",
    "            * ``lens`` array of episode length over collected episodes.\n",
    "            * ``idxs`` array of episode start index in buffer over collected episodes.\n",
    "            * ``rew`` mean of episodic rewards.\n",
    "            * ``len`` mean of episodic lengths.\n",
    "            * ``rew_std`` standard error of episodic rewards.\n",
    "            * ``len_std`` standard error of episodic lengths.\n",
    "        \"\"\"\n",
    "        assert not self.env.is_async, \"Please use AsyncCollector if using async venv.\"\n",
    "        if n_step is not None:\n",
    "            assert n_episode is None, (\n",
    "                f\"Only one of n_step or n_episode is allowed in Collector.\"\n",
    "                f\"collect, got n_step={n_step}, n_episode={n_episode}.\"\n",
    "            )\n",
    "            assert n_step > 0\n",
    "            if not n_step % self.env_num == 0:\n",
    "                warnings.warn(\n",
    "                    f\"n_step={n_step} is not a multiple of #env ({self.env_num}), \"\n",
    "                    \"which may cause extra transitions collected into the buffer.\"\n",
    "                )\n",
    "            ready_env_ids = np.arange(self.env_num)\n",
    "        elif n_episode is not None:\n",
    "            assert n_episode > 0\n",
    "            ready_env_ids = np.arange(min(self.env_num, n_episode))\n",
    "            self.data = self.data[:min(self.env_num, n_episode)]\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Please specify at least one (either n_step or n_episode) \"\n",
    "                \"in AsyncCollector.collect().\"\n",
    "            )\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        step_count = 0\n",
    "        episode_count = 0\n",
    "        episode_rews = []\n",
    "        episode_lens = []\n",
    "        episode_start_indices = []\n",
    "\n",
    "        while True:\n",
    "            assert len(self.data) == len(ready_env_ids)\n",
    "            # restore the state: if the last state is None, it won't store\n",
    "            last_state = self.data.policy.pop(\"hidden_state\", None)\n",
    "\n",
    "            # get the next action\n",
    "            if random:\n",
    "                try:\n",
    "                    act_sample = [\n",
    "                        self._action_space[i].sample() for i in ready_env_ids\n",
    "                    ]\n",
    "                except TypeError:  # envpool's action space is not for per-env\n",
    "                    act_sample = [self._action_space.sample() for _ in ready_env_ids]\n",
    "                act_sample = self.policy.map_action_inverse(act_sample)  # type: ignore\n",
    "                self.data.update(act=act_sample)\n",
    "            else:\n",
    "                if no_grad:\n",
    "                    with torch.no_grad():  # faster than retain_grad version\n",
    "                        # self.data.obs will be used by agent to get result\n",
    "                        result = self.policy(self.data.obs)\n",
    "                else:\n",
    "                    result = self.policy(self.data.obs)\n",
    "                # update state / act / policy into self.data\n",
    "                policy = result.get(\"policy\", Batch())\n",
    "                assert isinstance(policy, Batch)\n",
    "                state = result.get(\"state\", None)\n",
    "                if state is not None:\n",
    "                    policy.hidden_state = state  # save state into buffer\n",
    "                act = to_numpy(result.act)\n",
    "                if self.exploration_noise:\n",
    "                    act = self.policy.exploration_noise(act, self.data)\n",
    "                self.data.update(policy=policy, act=act)\n",
    "\n",
    "            # get bounded and remapped actions first (not saved into buffer)\n",
    "            action_remap = self.policy.map_action(self.data.act)\n",
    "            # step in env\n",
    "            result = self.env.step(action_remap, ready_env_ids)  # type: ignore\n",
    "            if len(result) == 5:\n",
    "                obs_next, rew, terminated, truncated, info = result\n",
    "                done = np.logical_or(terminated, truncated)\n",
    "            elif len(result) == 4:\n",
    "                obs_next, rew, done, info = result\n",
    "                if isinstance(info, dict):\n",
    "                    truncated = info[\"TimeLimit.truncated\"]\n",
    "                else:\n",
    "                    truncated = np.array(\n",
    "                        [\n",
    "                            info_item.get(\"TimeLimit.truncated\", False)\n",
    "                            for info_item in info\n",
    "                        ]\n",
    "                    )\n",
    "                terminated = np.logical_and(done, ~truncated)\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            self.data.update(\n",
    "                obs_next=obs_next,\n",
    "                rew=rew,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "                done=done,\n",
    "                info=info\n",
    "            )\n",
    "            if self.preprocess_fn:\n",
    "                self.data.update(\n",
    "                    self.preprocess_fn(\n",
    "                        obs_next=self.data.obs_next,\n",
    "                        rew=self.data.rew,\n",
    "                        done=self.data.done,\n",
    "                        info=self.data.info,\n",
    "                        policy=self.data.policy,\n",
    "                        env_id=ready_env_ids,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if render:\n",
    "                self.env.render()\n",
    "                if render > 0 and not np.isclose(render, 0):\n",
    "                    time.sleep(render)\n",
    "\n",
    "            # add data into the buffer\n",
    "            ptr, ep_rew, ep_len, ep_idx = self.buffer.add(\n",
    "                self.data, buffer_ids=ready_env_ids\n",
    "            )\n",
    "\n",
    "            # collect statistics\n",
    "            step_count += len(ready_env_ids)\n",
    "\n",
    "            if np.any(done):\n",
    "                env_ind_local = np.where(done)[0]\n",
    "                env_ind_global = ready_env_ids[env_ind_local]\n",
    "                episode_count += len(env_ind_local)\n",
    "                episode_lens.append(ep_len[env_ind_local])\n",
    "                episode_rews.append(ep_rew[env_ind_local])\n",
    "                episode_start_indices.append(ep_idx[env_ind_local])\n",
    "                # now we copy obs_next to obs, but since there might be\n",
    "                # finished episodes, we have to reset finished envs first.\n",
    "                self._reset_env_with_ids(\n",
    "                    env_ind_local, env_ind_global, gym_reset_kwargs\n",
    "                )\n",
    "                for i in env_ind_local:\n",
    "                    self._reset_state(i)\n",
    "\n",
    "                # remove surplus env id from ready_env_ids\n",
    "                # to avoid bias in selecting environments\n",
    "                if n_episode:\n",
    "                    surplus_env_num = len(ready_env_ids) - (n_episode - episode_count)\n",
    "                    if surplus_env_num > 0:\n",
    "                        mask = np.ones_like(ready_env_ids, dtype=bool)\n",
    "                        mask[env_ind_local[:surplus_env_num]] = False\n",
    "                        ready_env_ids = ready_env_ids[mask]\n",
    "                        self.data = self.data[mask]\n",
    "\n",
    "            self.data.obs = self.data.obs_next\n",
    "\n",
    "            if (n_step and step_count >= n_step) or \\\n",
    "                    (n_episode and episode_count >= n_episode):\n",
    "                break\n",
    "\n",
    "        # generate statistics\n",
    "        self.collect_step += step_count\n",
    "        self.collect_episode += episode_count\n",
    "        self.collect_time += max(time.time() - start_time, 1e-9)\n",
    "\n",
    "        if n_episode:\n",
    "            self.data = Batch(\n",
    "                obs={},\n",
    "                act={},\n",
    "                rew={},\n",
    "                terminated={},\n",
    "                truncated={},\n",
    "                done={},\n",
    "                obs_next={},\n",
    "                info={},\n",
    "                policy={}\n",
    "            )\n",
    "            self.reset_env()\n",
    "\n",
    "        if episode_count > 0:\n",
    "            rews, lens, idxs = list(\n",
    "                map(\n",
    "                    np.concatenate,\n",
    "                    [episode_rews, episode_lens, episode_start_indices]\n",
    "                )\n",
    "            )\n",
    "            rew_mean, rew_std = rews.mean(), rews.std()\n",
    "            len_mean, len_std = lens.mean(), lens.std()\n",
    "        else:\n",
    "            rews, lens, idxs = np.array([]), np.array([], int), np.array([], int)\n",
    "            rew_mean = rew_std = len_mean = len_std = 0\n",
    "\n",
    "        return {\n",
    "            \"n/ep\": episode_count,\n",
    "            \"n/st\": step_count,\n",
    "            \"rews\": rews,\n",
    "            \"lens\": lens,\n",
    "            \"idxs\": idxs,\n",
    "            \"rew\": rew_mean,\n",
    "            \"len\": len_mean,\n",
    "            \"rew_std\": rew_std,\n",
    "            \"len_std\": len_std,\n",
    "        }\n",
    "\n",
    "\n",
    "class AsyncCollector(Collector):\n",
    "    \"\"\"Async Collector handles async vector environment.\n",
    "\n",
    "    The arguments are exactly the same as :class:`~tianshou.data.Collector`, please\n",
    "    refer to :class:`~tianshou.data.Collector` for more detailed explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: BasePolicy,\n",
    "        env: BaseVectorEnv,\n",
    "        buffer: Optional[ReplayBuffer] = None,\n",
    "        preprocess_fn: Optional[Callable[..., Batch]] = None,\n",
    "        exploration_noise: bool = False,\n",
    "    ) -> None:\n",
    "        # assert env.is_async\n",
    "        warnings.warn(\"Using async setting may collect extra transitions into buffer.\")\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            buffer,\n",
    "            preprocess_fn,\n",
    "            exploration_noise,\n",
    "        )\n",
    "\n",
    "    def reset_env(self, gym_reset_kwargs: Optional[Dict[str, Any]] = None) -> None:\n",
    "        super().reset_env(gym_reset_kwargs)\n",
    "        self._ready_env_ids = np.arange(self.env_num)\n",
    "\n",
    "    def collect(\n",
    "        self,\n",
    "        n_step: Optional[int] = None,\n",
    "        n_episode: Optional[int] = None,\n",
    "        random: bool = False,\n",
    "        render: Optional[float] = None,\n",
    "        no_grad: bool = True,\n",
    "        gym_reset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Collect a specified number of step or episode with async env setting.\n",
    "\n",
    "        This function doesn't collect exactly n_step or n_episode number of\n",
    "        transitions. Instead, in order to support async setting, it may collect more\n",
    "        than given n_step or n_episode transitions and save into buffer.\n",
    "\n",
    "        :param int n_step: how many steps you want to collect.\n",
    "        :param int n_episode: how many episodes you want to collect.\n",
    "        :param bool random: whether to use random policy for collecting data. Default\n",
    "            to False.\n",
    "        :param float render: the sleep time between rendering consecutive frames.\n",
    "            Default to None (no rendering).\n",
    "        :param bool no_grad: whether to retain gradient in policy.forward(). Default to\n",
    "            True (no gradient retaining).\n",
    "        :param gym_reset_kwargs: extra keyword arguments to pass into the environment's\n",
    "            reset function. Defaults to None (extra keyword arguments)\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            One and only one collection number specification is permitted, either\n",
    "            ``n_step`` or ``n_episode``.\n",
    "\n",
    "        :return: A dict including the following keys\n",
    "\n",
    "            * ``n/ep`` collected number of episodes.\n",
    "            * ``n/st`` collected number of steps.\n",
    "            * ``rews`` array of episode reward over collected episodes.\n",
    "            * ``lens`` array of episode length over collected episodes.\n",
    "            * ``idxs`` array of episode start index in buffer over collected episodes.\n",
    "            * ``rew`` mean of episodic rewards.\n",
    "            * ``len`` mean of episodic lengths.\n",
    "            * ``rew_std`` standard error of episodic rewards.\n",
    "            * ``len_std`` standard error of episodic lengths.\n",
    "        \"\"\"\n",
    "        # collect at least n_step or n_episode\n",
    "        if n_step is not None:\n",
    "            assert n_episode is None, (\n",
    "                \"Only one of n_step or n_episode is allowed in Collector.\"\n",
    "                f\"collect, got n_step={n_step}, n_episode={n_episode}.\"\n",
    "            )\n",
    "            assert n_step > 0\n",
    "        elif n_episode is not None:\n",
    "            assert n_episode > 0\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Please specify at least one (either n_step or n_episode) \"\n",
    "                \"in AsyncCollector.collect().\"\n",
    "            )\n",
    "\n",
    "        ready_env_ids = self._ready_env_ids\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        step_count = 0\n",
    "        episode_count = 0\n",
    "        episode_rews = []\n",
    "        episode_lens = []\n",
    "        episode_start_indices = []\n",
    "\n",
    "        while True:\n",
    "            whole_data = self.data\n",
    "            self.data = self.data[ready_env_ids]\n",
    "            assert len(whole_data) == self.env_num  # major difference\n",
    "            # restore the state: if the last state is None, it won't store\n",
    "            last_state = self.data.policy.pop(\"hidden_state\", None)\n",
    "\n",
    "            # get the next action\n",
    "            if random:\n",
    "                try:\n",
    "                    act_sample = [\n",
    "                        self._action_space[i].sample() for i in ready_env_ids\n",
    "                    ]\n",
    "                except TypeError:  # envpool's action space is not for per-env\n",
    "                    act_sample = [self._action_space.sample() for _ in ready_env_ids]\n",
    "                act_sample = self.policy.map_action_inverse(act_sample)  # type: ignore\n",
    "                self.data.update(act=act_sample)\n",
    "            else:\n",
    "                if no_grad:\n",
    "                    with torch.no_grad():  # faster than retain_grad version\n",
    "                        # self.data.obs will be used by agent to get result\n",
    "                        result = self.policy(self.data, last_state)\n",
    "                else:\n",
    "                    result = self.policy(self.data, last_state)\n",
    "                # update state / act / policy into self.data\n",
    "                policy = result.get(\"policy\", Batch())\n",
    "                assert isinstance(policy, Batch)\n",
    "                state = result.get(\"state\", None)\n",
    "                if state is not None:\n",
    "                    policy.hidden_state = state  # save state into buffer\n",
    "                act = to_numpy(result.act)\n",
    "                if self.exploration_noise:\n",
    "                    act = self.policy.exploration_noise(act, self.data)\n",
    "                self.data.update(policy=policy, act=act)\n",
    "\n",
    "            # save act/policy before env.step\n",
    "            try:\n",
    "                whole_data.act[ready_env_ids] = self.data.act\n",
    "                whole_data.policy[ready_env_ids] = self.data.policy\n",
    "            except ValueError:\n",
    "                _alloc_by_keys_diff(whole_data, self.data, self.env_num, False)\n",
    "                whole_data[ready_env_ids] = self.data  # lots of overhead\n",
    "\n",
    "            # get bounded and remapped actions first (not saved into buffer)\n",
    "            action_remap = self.policy.map_action(self.data.act)\n",
    "            # step in env\n",
    "            result = self.env.step(action_remap, ready_env_ids)  # type: ignore\n",
    "\n",
    "            if len(result) == 5:\n",
    "                obs_next, rew, terminated, truncated, info = result\n",
    "                done = np.logical_or(terminated, truncated)\n",
    "            elif len(result) == 4:\n",
    "                obs_next, rew, done, info = result\n",
    "                if isinstance(info, dict):\n",
    "                    truncated = info[\"TimeLimit.truncated\"]\n",
    "                else:\n",
    "                    truncated = np.array(\n",
    "                        [\n",
    "                            info_item.get(\"TimeLimit.truncated\", False)\n",
    "                            for info_item in info\n",
    "                        ]\n",
    "                    )\n",
    "                terminated = np.logical_and(done, ~truncated)\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            # change self.data here because ready_env_ids has changed\n",
    "            try:\n",
    "                ready_env_ids = info[\"env_id\"]\n",
    "            except Exception:\n",
    "                ready_env_ids = np.array([i[\"env_id\"] for i in info])\n",
    "            self.data = whole_data[ready_env_ids]\n",
    "\n",
    "            self.data.update(\n",
    "                obs_next=obs_next,\n",
    "                rew=rew,\n",
    "                terminated=terminated,\n",
    "                truncated=truncated,\n",
    "                info=info\n",
    "            )\n",
    "            if self.preprocess_fn:\n",
    "                try:\n",
    "                    self.data.update(\n",
    "                        self.preprocess_fn(\n",
    "                            obs_next=self.data.obs_next,\n",
    "                            rew=self.data.rew,\n",
    "                            terminated=self.data.terminated,\n",
    "                            truncated=self.data.truncated,\n",
    "                            info=self.data.info,\n",
    "                            env_id=ready_env_ids,\n",
    "                        )\n",
    "                    )\n",
    "                except TypeError:\n",
    "                    self.data.update(\n",
    "                        self.preprocess_fn(\n",
    "                            obs_next=self.data.obs_next,\n",
    "                            rew=self.data.rew,\n",
    "                            done=self.data.done,\n",
    "                            info=self.data.info,\n",
    "                            env_id=ready_env_ids,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            if render:\n",
    "                self.env.render()\n",
    "                if render > 0 and not np.isclose(render, 0):\n",
    "                    time.sleep(render)\n",
    "\n",
    "            # add data into the buffer\n",
    "            ptr, ep_rew, ep_len, ep_idx = self.buffer.add(\n",
    "                self.data, buffer_ids=ready_env_ids\n",
    "            )\n",
    "\n",
    "            # collect statistics\n",
    "            step_count += len(ready_env_ids)\n",
    "\n",
    "            if np.any(done):\n",
    "                env_ind_local = np.where(done)[0]\n",
    "                env_ind_global = ready_env_ids[env_ind_local]\n",
    "                episode_count += len(env_ind_local)\n",
    "                episode_lens.append(ep_len[env_ind_local])\n",
    "                episode_rews.append(ep_rew[env_ind_local])\n",
    "                episode_start_indices.append(ep_idx[env_ind_local])\n",
    "                # now we copy obs_next to obs, but since there might be\n",
    "                # finished episodes, we have to reset finished envs first.\n",
    "                self._reset_env_with_ids(\n",
    "                    env_ind_local, env_ind_global, gym_reset_kwargs\n",
    "                )\n",
    "                for i in env_ind_local:\n",
    "                    self._reset_state(i)\n",
    "\n",
    "            try:\n",
    "                whole_data.obs[ready_env_ids] = self.data.obs_next\n",
    "                whole_data.rew[ready_env_ids] = self.data.rew\n",
    "                whole_data.done[ready_env_ids] = self.data.done\n",
    "                whole_data.info[ready_env_ids] = self.data.info\n",
    "            except ValueError:\n",
    "                _alloc_by_keys_diff(whole_data, self.data, self.env_num, False)\n",
    "                self.data.obs = self.data.obs_next\n",
    "                whole_data[ready_env_ids] = self.data  # lots of overhead\n",
    "            self.data = whole_data\n",
    "\n",
    "            if (n_step and step_count >= n_step) or \\\n",
    "                    (n_episode and episode_count >= n_episode):\n",
    "                break\n",
    "\n",
    "        self._ready_env_ids = ready_env_ids\n",
    "\n",
    "        # generate statistics\n",
    "        self.collect_step += step_count\n",
    "        self.collect_episode += episode_count\n",
    "        self.collect_time += max(time.time() - start_time, 1e-9)\n",
    "\n",
    "        if episode_count > 0:\n",
    "            rews, lens, idxs = list(\n",
    "                map(\n",
    "                    np.concatenate,\n",
    "                    [episode_rews, episode_lens, episode_start_indices]\n",
    "                )\n",
    "            )\n",
    "            rew_mean, rew_std = rews.mean(), rews.std()\n",
    "            len_mean, len_std = lens.mean(), lens.std()\n",
    "        else:\n",
    "            rews, lens, idxs = np.array([]), np.array([], int), np.array([], int)\n",
    "            rew_mean = rew_std = len_mean = len_std = 0\n",
    "\n",
    "        return {\n",
    "            \"n/ep\": episode_count,\n",
    "            \"n/st\": step_count,\n",
    "            \"rews\": rews,\n",
    "            \"lens\": lens,\n",
    "            \"idxs\": idxs,\n",
    "            \"rew\": rew_mean,\n",
    "            \"len\": len_mean,\n",
    "            \"rew_std\": rew_std,\n",
    "            \"len_std\": len_std,\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
