# -*- coding: utf-8 -*-
"""VRP_training_raytune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aPi7iQp7gj3HNYpUnqoeVSv__ggkrHwv
"""

# Importing required libraries

import os 
import sys
import random
import argparse
import warnings
warnings.simplefilter('ignore')
from functools import partial

import gym
from gym import spaces

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import pickle
from ray import tune

import torch
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter

# Modules from tianshou framework

import tianshou
from typing import Any, Callable, List, Optional, Tuple, Union, Dict
from tianshou.env import DummyVectorEnv
from tianshou.data import Batch, to_torch, to_torch_as
from tianshou.policy import BasePolicy
from tianshou.utils import TensorboardLogger


from tianshou.env.worker import (
    DummyEnvWorker,
    EnvWorker,
    RayEnvWorker,
    SubprocEnvWorker,
)

# Derived modules and custom defined classes

from env.VRPEnv import VRPEnv
from policy.VRPPolicy import REINFORCEPolicy
from nets.attention_model import AttentionModel

from data.VRPCollector import Collector
from data.BufferManager import ReplayBuffer, VectorReplayBuffer
from policy.VRPtrainer import OnpolicyTrainer, onpolicy_trainer
from data.Graph_Viz import decode_buffer, plot_vehicle_routes


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if device == "cuda":
    torch.cuda.get_device_properties(device)

## Parameters of dataset
def load_data(data_dir):
    """
    Load the saved dataset/graphs
    """

    train_data_path = data_dir + "/train/train_graphs_50000x10.pickle"
    test_data_path = data_dir + "/test/test_graphs_50000x10.pickle"

    with open(train_data_path, 'rb') as train_handle:
        load_train_graphs = pickle.load(train_handle)

    with open(test_data_path, 'rb') as test_handle:
        load_test_graphs = pickle.load(test_handle) 
        
    return load_train_graphs, load_test_graphs



def train_model(data_dir=None, checkpoint_dir='./'):
    """
    Wrapper for training
    """
    
    load_train_graphs, load_test_graphs = load_data(data_dir)
    
    # Training parameters
    training_params = {
    "experiment_idx" : "D50x10",
    
    # Optimization
    "learning_rate" : 0.0001,
    "betas" : (0.9, 0.99), # coefficients used for computing running averages of gradient and its square
    "weight_decay" : 0.0001,  # weight decay coefficient for regularization
    "n_epochs" : 50,
    "batch_size" : 256,
    
    #Model configuration
    "embedding_dim" : 64,
    "hidden_dim" : 16,
    "n_encode_layers" : 2,
    
    # Trainer and Collector setup (will remain almost same, increase buffer sizes for larger datasets)
    "graph_size" : load_train_graphs[0]["node_features"].shape[0] - 1,
    "train_graphs" : len(load_train_graphs),
    "test_graphs" : len(load_test_graphs),
    "train_buffer_size" : 1000000,
    "test_buffer_size" : 1000000,
    "repeat_per_collect" : 1,
    "test_in_train" : True}
    
    training_params["episode_per_collect"] = training_params["episode_per_test"] = training_params["train_graphs"]
    training_params["step_per_epoch"] = training_params["graph_size"] * training_params["train_graphs"]
    
    print(training_params)
    
    
    model = AttentionModel(
        embedding_dim = training_params["embedding_dim"],
        hidden_dim = training_params["hidden_dim"],
        n_encode_layers = training_params["n_encode_layers"],
        graph_size = training_params["graph_size"],
        tanh_clipping = 10,
        mask_inner = True, 
        mask_logits = True,
        normalization = 'batch',
        n_heads = 8,
        checkpoint_encoder = False,
        shrink_size = None)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    
    optim = torch.optim.AdamW(
        model.parameters(), 
        lr = training_params["learning_rate"],
        betas = training_params["betas"], 
        weight_decay = training_params["weight_decay"],
        eps = 1e-08)
    
    VRPpolicy = REINFORCEPolicy(model, optim)
    
    
    # Setting up Vectorized environments for train and test datasets
    train_envs = DummyVectorEnv([lambda instance=graph, idx=i: VRPEnv(instance, idx) for i,graph in enumerate(load_train_graphs)])
    test_envs = DummyVectorEnv([lambda instance=graph, idx=i: VRPEnv(instance, idx) for i,graph in enumerate(load_test_graphs)])
    
    # Setting up Replay Buffers and Collectors
    test_replaybuffer = VectorReplayBuffer(training_params["test_buffer_size"], buffer_num=training_params["test_graphs"])
    train_replaybuffer = VectorReplayBuffer(training_params["train_buffer_size"], buffer_num=training_params["train_graphs"])

    test_collector = Collector(VRPpolicy, test_envs, test_replaybuffer)
    train_collector = Collector(VRPpolicy, train_envs, train_replaybuffer)
    
    # Setting up trainer 
    logdir = "./logs/"
    exp_num = training_params["experiment_idx"]

    # Setup Tensorboard logger
    log_path = os.path.join(logdir, f"VRPtraining_exp{exp_num}")
    writer = SummaryWriter(log_path)
    logger = TensorboardLogger(writer)


    train_collector.reset()
    test_collector.reset()
    train_replaybuffer.reset()
    test_replaybuffer.reset()

    trainer = OnpolicyTrainer(
        VRPpolicy,
        train_collector,
        test_collector,
        max_epoch = training_params["n_epochs"],
        step_per_epoch = training_params["step_per_epoch"],
        repeat_per_collect = training_params["repeat_per_collect"],
        episode_per_test = training_params["episode_per_test"],
        episode_per_collect = training_params["episode_per_collect"],
        batch_size = training_params["batch_size"],
        logger=logger)
    
    
    # Train the model and store epoch stats in a dataframe
    losses = []
    train_stat = []

    for epoch, epoch_stat, info in trainer:
        losses.append(-epoch_stat["loss"])
        epoch_stat["epoch"] = epoch
        train_stat.append(epoch_stat)
        print(epoch_stat)
        
        with tune.checkpoint_dir(epoch) as checkpoint_dir:
            path = os.path.join(checkpoint_dir, "checkpoints")
            torch.save((model.state_dict(), optim.state_dict()), path)

    train_df_cols = epoch_stat.keys()
    train_df = pd.DataFrame(train_stat, columns = train_df_cols)
    
    print("Finished Training")
    
    
    # Saving the trained model
    exp_idx = f"model_exp{exp_num}_g{training_params['graph_size']}_train{len(load_train_graphs)}_test{len(load_test_graphs)}"
    file_path = "./trained_models/models/" + f"{exp_idx}.pth"
    torch.save(trained_model.state_dict(), file_path)
    
    
    # Collecting test and train results and buffers data
    train_result = train_collector.collect(n_episode=len(load_train_graphs))
    test_result = test_collector.collect(n_episode=len(load_test_graphs))
    train_buffer_df = decode_buffer(train_replaybuffer)
    train_buffer_df = decode_buffer(train_replaybuffer)
    
    print("\n\n")
    print(f"Train Results: {test_result}")
    print(f"Test Results: {test_result}")
    
    res_path = "./trained_models/results/"
    train_df.to_csv(res_path + f"train_df_{exp_idx}", index=False)
    train_buffer_df.to_csv(res_path + f"train_bufferdf_{exp_idx}", index=False)
    test_buffer_df.to_csv(res_path + f"test_bufferdf_{exp_idx}", index=False)
    
    
    with open(res_path + f"test_result_{exp_idx}.pickle", 'wb') as handle: 
            pickle.dump(test_result, handle, protocol=pickle.HIGHEST_PROTOCOL)
            
    
    with open(res_path + f"train_result_{exp_idx}.pickle", 'wb') as handle: 
            pickle.dump(train_result, handle, protocol=pickle.HIGHEST_PROTOCOL)
            
    with open(res_path + f"params_{exp_idx}.pickle", 'wb') as handle: 
            pickle.dump(training_params, handle, protocol=pickle.HIGHEST_PROTOCOL)
    
    return 


def main():
    
    data_dir = os.path.abspath("./data/")
    result = train_model(data_dir)

if __name__ == "__main__":
    main()

